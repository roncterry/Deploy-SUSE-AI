
############################################################################
#   Common Variables
############################################################################

# Namspace that Will Contain All SUSE AI Objects
#
export SUSE_AI_NAMESPACE=suse-ai


# Name of the Storgae Class to Use When One is Needed
#
export STORAGE_CLASS_NAME=longhorn


# URI to the SUSE Rancher Application Collection (App Co)
#
export APP_COLLECTION_URI=oci://dp.apps.rancher.io/charts


# The Name of the Secret to Create that is Used to Access the App Co
#
export IMAGE_PULL_SECRET_NAME=application-collection


############################################################################
#   RKE2/K3S Variables
############################################################################

# K8s distro to deploy
#
# Options: rke2, k3s
#
# Default (recommended): rke2
#
export K8S_DISTRO=rke2


# K8s distro version channel to deploy from
#
# This is the channel containing the version of K8s (RKE2/K3S) to deploy.
#
# If you need a specific version you must supply the version tag.
#   Example: v1.30
# 
# If a spcific version tag is not specified then you must supply one of 
# the following version tags: stable, latest, testing
#   Example: stable
#
# Default: stable
#
export K8S_DISTRO_CHANNEL=v1.30


# Values Used for the Cluster Token
#
# Also commonly used as the na,e fo the cluster when configuring name
# resolution or when importing the cluster into Rancher Manager.
#
export CLUSTER_NAME=aicluster01


# DNS Domain Name
#
# DNS domain name of the cluster nodes and the cluster
#
# THe AI cluster is assumed to have a DNS entry that resolves 
# CLUSTER_NAME.DOMAIN_NAME to the IP address of the first cluster node 
# (or round-robin DNS/loadbalancer to all cluster control nodes or cluster VIP) 
#
export DOMAIN_NAME=example.com


############################################################################
#   NVIDIA GPU Operator Variables
############################################################################

# URL of the NVIDIA GPU Operator Helm Repository
#
export NVIDIA_GPU_OPERATOR_REPO_URL=https://helm.ngc.nvidia.com/nvidia


############################################################################
#   SUSE Storage (Longhorn) Variables
############################################################################

# URL of the Longhorn Helm Repository
#
export LH_HELM_REPO_URL=https://charts.longhorn.io


# Username for the Longhorn Admin User
#
export LH_USER="admin"


# Password for the Longhorn Admin User
#
export LH_PASSWORD="longhorn"


# URL to Access the Longhorn Web UI If Configuring an Ingress
#
export LH_URL="${CLUSTER_NAME}-longhorn.${DOMAIN_NAME}"


# Number of Replicas of Longhorn Service Pods
#
# This typically is set to the number of cluster nodes in the Longhorn cluster.
# For example, if you have a single node cluster this should be set to: 1
#
# This is used for the following Helm chart values:
#   defaultSettings:defaultReplicaCount
#
# Default: 3
#
export LH_DEFAULT_REPLICA_COUNT=1


# The Number of Volume Replicas in the Default Storage Class
#
# When the defualt storage class is created (default name: longhorn) when
# Longhorn is deployed, this is the default number of volume replicas that
# will be defined in that storage class.
#
# This is typically set to the number of cluster nodes if less than 3.
# For example, if you have a single node cluster this should be set to: 1
#
# This is used for the following Helm chart values:
#   persistence:defaultClassReplicaCount
#
# Default: 3
#
export LH_DEFAULT_CLASS_REPLICA_COUNT=1


# The Number of CSI Service Pods
#
# This typically is set to the number of cluster nodes in the Longhorn cluster.
# For example, if you have a single node cluster this should be set to: 1
#
# This is used for the following Helm chart values:
#    csi:attacherReplicaCount
#    csi:provisionerReplicaCount
#    csi:resizerReplicaCount
#    csi:snapshotterReplicaCount
#
# Default: 3
#
export LH_CSI_REPLICA_COUNT=1


# The Percentage of the Default Disk that is Not to Be Allocated
#
# This defines the percentage of the default disk that will not be allocated
# for use as Longhorn storage.
#
# For example, if you have a 1TB disk and if your root volume (/) is 850GB and 
# the percentage is set to 30, only roughly 550GB will be used for Longhorn 
# storage leaving roughly 250GB for use by other things (OS, etc.).
#
# If the first disk is the same disk that your OS is installed on you might 
# want to watch this. However if that disk in 1TB, 30 is probably a little too 
# much to reserve. You might consider setting this to a lower value like 20 or 
# even 15.
#
# This is used for the following Helm chart values:
#   defaultSettings:storageReservedPercentageForDefaultDisk
#
# Default: 30
#
export LH_RESERVED_DISK_PERCENTAGE=15


############################################################################
#   SUSE Security Variables
############################################################################

# Namespace to Install SUSE Security (NeuVector) Into
#
export SECURITY_NAMESPACE=cattle-neuvector-system


# The URL for the SUSE Security (NeuVector) Helm Repository
#
export SECURITY_HELM_REPO_URL="https://neuvector.github.io/neuvector-helm/"


# The Number of Replicas for the Controller and CVE Scanner
#
# This is used for the following Helm chart values:
#   controller:replicas
#   cve:scanner:replicas
#
# Default: 3
#
export SECURITY_REPLICAS=1


############################################################################
#   Milvus Variables
############################################################################

# Version of Milvus to Install
#
# If left empty the latest version will be installed.
#
export MILVUS_VERSION=


# Run Milvus in clustered or standalone mode
#
# If there is only a single cluster node set this to: false
#
# If you have multiple cluster nodes but still want to run in standalone mode
# set this to: false
#
# If set to "false" the only containers that are deployed are:
#  -milvus-etcd
#  -milvus-minio
#  -milvus-standalone
#
# Default: true
#
export MILVUS_CLUSTER_ENABLED=false


# Log level to use with Milvus
#
# Default: info
#
export MILVUS_LOGGING_LEVEL=info


# Where to store the Milvus logs
#
# Options:
#    emptyDir      (store logs in an emptyDir)
#    storageClass  (store logs in a PV of type storageClass)
#
# Defalult: emptyDir
#
export MILVUS_LOGGING_STORAGE=emptyDir


# Message queue to use with Milvus in Standalone Mode
#
# Standalone Mode:
#   Default: rocksmq
# 
#   Milvus can use kafka in standalone mode however,
#   if in doubt leave this set to: rocksmq
#
# Cluster Mode:
#   Default: kafka
#
export MILVUS_STANDALONE_MESSAGE_QUEUE=rocksmq


#========  Etcd Values  ========#

# Enable etcd
#
# Default: true
#
export MILVUS_ETCD_ENABLED=true


# Etcd Replica Count
#
# Set this t "1" if runing a single node cluster or running in standalone mode.
#
# Default: 3
#
export MILVUS_ETCD_REPLICA_COUNT=1


#======== Minio Values  ========#

# Enable MinIO
#
# Default: true
#
export MILVUS_MINIO_ENABLED=true


# Minio Admin Username
#
export MILVUS_MINIO_ROOT_USER=admin


# Minio Admin User Password
#
export MILVUS_MINIO_ROOT_USER_PASSWORD=adminminio


# Minio Mode
#
# Options: distributed, standalone
#
# Set this to "standalone" if MILVUS_CLUSTER_ENABLED=False
#
# Default: distributed
#
export MILVUS_MINIO_MODE=standalone


# Minio Replica Count
#
# Set this to "1" if running in stadalone mode.
#
# Default: 4
#
export MILVUS_MINIO_REPLICA_COUNT=1


# Volume Size for the Minio Persistent Volume
#
# Default: 500Gi
#
export MILVUS_MINIO_VOLUME_SIZE=100Gi


# The Amount of Memory that Minio Uses
#
# Default: 1024Mi
#
export MILVUS_MINIO_MEMORY=4096Mi


#========  Kafka Values  ========#

# Enable Kafka
#
# For single node/standalone Milvus Kafka is disabled (set to false)
#
# Default: true
#
export MILVUS_KAFKA_ENABLED=false


# Kaufka Replica Count
#
# Default: 3
#
export MILVUS_KAFKA_REPLICA_COUNT=3


# Kafka Volume Size
#
#Default: 8Gi
#
export MILVUS_KAFKA_VOLUME_SIZE=8Gi


############################################################################
#   Ollama Variables
############################################################################

# Version of Ollama to Install
#
# If left empty the latest version will be installed.
#
export OLAMA_VERSION=


# URL to Access the Ollama API if Configuring an Ingress
#
export OLLAMA_INGRESS_HOST=${CLUSTER_NAME}.${DOMAIN_NAME}


# First LLM for Ollama to Download
#
# At minimum this must be set so that a model is downloaded during deployment.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_0=llama3.2


# Second LLM for Ollama to Download
#
# Leave empty if you do not want an addtional model.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_1=gemma:2b


# Third LLM for Ollama to Download
#
# Leave empty if you do not want an addtional model.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_2=


# Fourth LLM for Ollama to Download
#
# Leave empty if you do not want an addtional model.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_3=


# Fifth LLM for Ollama to Download
#
# Leave empty if you do not want an addtional model.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_4=


############################################################################
#   Open WebUI Variables
############################################################################

# Version of Open WebUI to Install
#
# If left empty the latest version will be installed.
#
export OWUI_VERSION=


# Enable Open WebUI to install Ollama
#
# If enabled (set to True) Ollama will be deployed as part of the Open WebUI 
# deployment and that is the instance Open WebUI will use.
# If Ollama was previous deployed separatly and you want Open WebUI to use it
# instead of an instance it deploys set this to False.
#
# Default: True
#
export OWUI_OLLAMA_ENABLED=true


# URL Used to Access the Open WebUI Web Interface
#
export WEBUI_INGRESS_HOST=${CLUSTER_NAME}.${DOMAIN_NAME}


