
############################################################################
#   Common Variables
############################################################################

# Namspace that Will Contain All SUSE AI Objects
#
export SUSE_AI_NAMESPACE=suse-ai


# Name of the Storgae Class to Use When One is Needed
#
export STORAGE_CLASS_NAME=longhorn


# URI to the SUSE Rancher Application Collection (App Co)
#
export APP_COLLECTION_URI=oci://dp.apps.rancher.io/charts


# The Name of the Secret to Create that is Used to Access the App Co
#
export IMAGE_PULL_SECRET_NAME=application-collection


############################################################################
#   RKE2/K3S Variables
############################################################################

# K8s distro to deploy
#
# Options: rke2, k3s
#
# Default (recommended): rke2
#
export K8S_DISTRO=rke2


# K8s distro version channel to deploy from
#
# This is the channel containing the version of K8s (RKE2/K3S) to deploy.
#
# If you need a specific version you must supply the version tag.
#   Example: v1.30
# 
# If a spcific version tag is not specified then you must supply one of 
# the following version tags: stable, latest, testing
#   Example: stable
#
# Default: stable
#
export K8S_DISTRO_CHANNEL=v1.30


# Value Used for the Hostname of the Cluster
#
# Used as the name of the cluster when configuring name resolution or also
# commonly used as the cluster name when importing the cluster into Rancher Manager.
#
export CLUSTER_NAME=aicluster01


# Value Used for the Cluster Token
#
export CLUSTER_TOKEN=${CLUSTER_NAME}


# DNS Domain Name
#
# DNS domain name of the cluster nodes and the cluster
#
# The cluster is assumed to have a DNS entry that resolves 
# CLUSTER_NAME.DOMAIN_NAME to the IP address of the first cluster node 
# (or round-robin DNS/loadbalancer to all cluster control nodes or cluster VIP) 
#
export DOMAIN_NAME=example.com


# Disable Built-in RKE services
#
# Space-delimited list
#
# If you want to disable any of the built-in RKE2 services, add them to this
# variable.
#
# For example if you want to install an external ingress controller rather than
# have RKE2 install the built-in rke2-ingress-nginx controller add 
# 'rke2-ingress-nginx' to the variable. 
#
# (Note: This is usefull if you want to install kube-vip to configure a cluster 
#  virtual IP which requires configuration of ingress-nginx separatly instead 
#  of using the built-in ingress-nginx.)
#
export DISABLED_BUILTIN_SERVICE_LIST=


# Install and External Ingress Controller
#
# Options: true|false
#
# Default: false
#
# Note: If INSTALL_EXTERNAL_INGRESS_CONTROLLER=true then 
#       DISABLE_BUILTIN_SERVICE_LIST must contain 'rke2-ingress-nginx'
#       to ensure the built-in ingress controller is not enabled.
#
#       This must be set to 'true' if INSTALL_KUBEVIP=true
#
export INSTALL_EXTERNAL_INGRESS_CONTROLLER=false


# Namespace to Install the External Ingress Controller Into
#
# Default: kube-system
#
# The namespace where the built-in ingress controller is installed is 
# kube-system. Installing into that namespace avoids certificate issues.
#
export EXTERNAL_INGRESS_CONTROLLER_NAMESPACE=kube-system


# How to Deploy the External Ingress Controller
#
# Options: Deployment|DaemonSet
#
# Default: Deployment
#
# When set to DaemonSet the number of replicas does not need to be set becase
# a replica will be created on each node in the cluster.
#
export EXTERNAL_INGRESS_CONTROLLER_KIND=DaemonSet


# Number of External Ingress Controller Repicas
#
# Default: 1
#
# This only has effect if EXTERNAL_INGRESS_CONTROLLER_TYPE=Deployment
#
export EXTERNAL_INGRESS_CONTROLLER_REPLICAS=1


# Install kube-vip to Configure a Cluster Virtual IP
#
# Options: true|false
#
# Default: false
#
# Note: If INSTALL_KUBEVIP=true then you must also set:
#       INSTALL_EXTERNAL_INGRESS_CONTROLLER=true
#       and
#       DISABLE_BUILTIN_SERVICE_LIST must include 'rke2-ingress-nginx'
#
#       You must also set a virtual IP in RKE2_CLUSTER_VIP
# 
export INSTALL_KUBEVIP=false


# Virtual IP for the RKE2 cluster
#
#  Note: If a virtual IP is set here then you must also set:
#       INSTALL_EXTERNAL_INGRESS_CONTROLLER=true
#       and
#       DISABLE_BUILTIN_SERVICE_LIST must include 'rke2-ingress-nginx'
#
# Example: 172.31.9.200/32
#
export RKE2_CLUSTER_VIP=


############################################################################
#   NVIDIA GPU Operator Variables
############################################################################

# URL of the NVIDIA GPU Operator Helm Repository
#
export NVIDIA_GPU_OPERATOR_REPO_URL=https://helm.ngc.nvidia.com/nvidia


############################################################################
#   SUSE Storage (Longhorn) Variables
############################################################################

# URL of the Longhorn Helm Repository
#
export LH_HELM_REPO_URL=https://charts.longhorn.io


# Username for the Longhorn Admin User
#
export LH_USER="admin"


# Password for the Longhorn Admin User
#
export LH_PASSWORD="longhorn"


# URL to Access the Longhorn Web UI If Configuring an Ingress
#
export LH_URL="${CLUSTER_NAME}-longhorn.${DOMAIN_NAME}"


# Number of Replicas of Longhorn Service Pods
#
# This typically is set to the number of cluster nodes in the Longhorn cluster.
# For example, if you have a single node cluster this should be set to: 1
#
# This is used for the following Helm chart values:
#   defaultSettings:defaultReplicaCount
#
# Default: 3
#
export LH_DEFAULT_REPLICA_COUNT=1


# The Number of Volume Replicas in the Default Storage Class
#
# When the defualt storage class is created (default name: longhorn) when
# Longhorn is deployed, this is the default number of volume replicas that
# will be defined in that storage class.
#
# This is typically set to the number of cluster nodes if less than 3.
# For example, if you have a single node cluster this should be set to: 1
#
# This is used for the following Helm chart values:
#   persistence:defaultClassReplicaCount
#
# Default: 3
#
export LH_DEFAULT_CLASS_REPLICA_COUNT=1


# The Number of CSI Service Pods
#
# This typically is set to the number of cluster nodes in the Longhorn cluster.
# For example, if you have a single node cluster this should be set to: 1
#
# This is used for the following Helm chart values:
#    csi:attacherReplicaCount
#    csi:provisionerReplicaCount
#    csi:resizerReplicaCount
#    csi:snapshotterReplicaCount
#
# Default: 3
#
export LH_CSI_REPLICA_COUNT=1


# The Percentage of the Default Disk that is Not to Be Allocated
#
# This defines the percentage of the default disk that will not be allocated
# for use as Longhorn storage.
#
# For example, if you have a 1TB disk and if your root volume (/) is 850GB and 
# the percentage is set to 30, only roughly 550GB will be used for Longhorn 
# storage leaving roughly 250GB for use by other things (OS, etc.).
#
# If the first disk is the same disk that your OS is installed on you might 
# want to watch this. However if that disk in 1TB, 30 is probably a little too 
# much to reserve. You might consider setting this to a lower value like 20 or 
# even 15.
#
# This is used for the following Helm chart values:
#   defaultSettings:storageReservedPercentageForDefaultDisk
#
# Default: 30
#
export LH_RESERVED_DISK_PERCENTAGE=15


# Default Storage Class File System Type
#
# The file system type used in the default Storage Class is explicitly 
# defined by this variable. The default vallue in the chart is ext4 however 
# there are times when somthing like XFS would be better.
#
# !----This value must be set----!
# (If in doubt set it to: ext4)
#
# Default: ext4
#
export LH_DEFAULT_SC_FS_TYPE=ext4


# List of additional storage classes to create
#
# If you need storage class(es) that use a different file system and/or 
# replica count than the default Longhorn storage class you can list them 
# in this variable and a new storage class will be created for each 
# one listed. 
#
# Note: Only file systems supported by Longhorn can be listed
#
# Currently supported file systems: ext4 xfs
#
# Space-delimited list of comma delimited lists
#
# Format: <NAME>,<FS_TYPE>,<REPLICAS>
#
# Example: longhorn-xfs,xfs,1 longhorn-ext4,ext4,3 
#
# Ths example would create:
#  -an additional storage class using XFS as the file system with 1 replica
#   named: longhorn-xfs
#  -an additional storage class using EXT4 as the file system with 3 replicas
#   named: longhorn-ext4
#
export LH_ADDITIONAL_SC_LIST=


############################################################################
#   SUSE Security Variables
############################################################################

# Namespace to Install SUSE Security (NeuVector) Into
#
export SECURITY_NAMESPACE=cattle-neuvector-system


# The URL for the SUSE Security (NeuVector) Helm Repository
#
export SECURITY_HELM_REPO_URL="https://neuvector.github.io/neuvector-helm/"


# The Number of Replicas for the Controller and CVE Scanner
#
# This is used for the following Helm chart values:
#   controller:replicas
#   cve:scanner:replicas
#
# Default: 3
#
export SECURITY_REPLICAS=1


# Password for the SUSE Security Admin User
#
# Admin username: admin
# Default password: admin
#
export SECURITY_ADMIN_PW=security


# Enabled Rancher Single Sign On
#
# Must be set to 'true' if single sign on login into the NeuVector web UI
# is desired. A security account named 'neuvector' must also be created in
# the ${SECURITY_NAMESPACE} for SSO to work.
# 
# Default: false
#
export SECURITY_RANCHERSSO_ENABLED=true


# Create an ingress for the NeuVector Controller
#
# If set to 'true' an ingress will be created using the following as the URL 
# for the ingress:
#   ${CLUSTER_NAME}.${DOMAIN_NAME}/security
#
# Default: false
#
export SECURITY_INGRESS_ENABLED=false


############################################################################
#   Milvus Variables
############################################################################

# Version of Milvus to Install
#
# If left empty the latest version will be installed.
#
export MILVUS_VERSION=


# Run Milvus in clustered or standalone mode
#
# If there is only a single cluster node set this to: false
#
# If you have multiple cluster nodes but still want to run in standalone mode
# set this to: false
#
# If set to "false" the only containers that are deployed are:
#  -milvus-etcd
#  -milvus-minio
#  -milvus-standalone
#
# Default: true
#
export MILVUS_CLUSTER_ENABLED=false


# Log level to use with Milvus
#
# Default: info
#
export MILVUS_LOGGING_LEVEL=info


# Where to store the Milvus logs
#
# Options:
#    emptyDir      (store logs in an emptyDir)
#    storageClass  (store logs in a PV of type storageClass)
#
# Defalult: emptyDir
#
export MILVUS_LOGGING_STORAGE=emptyDir


# Message queue to use with Milvus in Standalone Mode
#
# Standalone Mode:
#   Default: rocksmq
# 
#   Milvus can use kafka in standalone mode however,
#   if in doubt leave this set to: rocksmq
#
# Cluster Mode:
#   Default: kafka
#
export MILVUS_STANDALONE_MESSAGE_QUEUE=rocksmq


#========  Etcd Values  ========#

# Enable etcd
#
# Default: true
#
export MILVUS_ETCD_ENABLED=true


# Etcd Replica Count
#
# Set this t "1" if runing a single node cluster or running in standalone mode.
#
# Default: 3
#
export MILVUS_ETCD_REPLICA_COUNT=1


#======== Minio Values  ========#

# Enable MinIO
#
# Default: true
#
export MILVUS_MINIO_ENABLED=true


# Minio Admin Username
#
export MILVUS_MINIO_ROOT_USER=admin


# Minio Admin User Password
#
export MILVUS_MINIO_ROOT_USER_PASSWORD=adminminio


# Minio Mode
#
# Options: distributed, standalone
#
# Set this to "standalone" if MILVUS_CLUSTER_ENABLED=False
#
# Default: distributed
#
export MILVUS_MINIO_MODE=standalone


# Minio Replica Count
#
# Set this to "1" if running in stadalone mode.
#
# Default: 4
#
export MILVUS_MINIO_REPLICA_COUNT=1


# Volume Size for the Minio Persistent Volume
#
# Default: 500Gi
#
export MILVUS_MINIO_VOLUME_SIZE=100Gi


# The Amount of Memory that Minio Uses
#
# Default: 1024Mi
#
export MILVUS_MINIO_MEMORY=4096Mi


#========  Kafka Values  ========#

# Enable Kafka
#
# For single node/standalone Milvus Kafka is disabled (set to false)
#
# Default: true
#
export MILVUS_KAFKA_ENABLED=false


# Kaufka Replica Count
#
# Default: 3
#
export MILVUS_KAFKA_REPLICA_COUNT=3


# Kafka Volume Size
#
#  Default: 8Gi
#
export MILVUS_KAFKA_VOLUME_SIZE=8Gi


# Kafka Storage Class Name
#
# The name of the storage class that Kafka will use when creating 
# persistent volumes.
#
# Kafka PVs must be formatted with a file system other than Ext4, such as XFS.
#
# The default Longhorn storage class typically uses the Ext4 filesystem so a 
# new storage class must be created that uses XFS instead (unless the default 
# storage class was configured to use XFS). If this new storage class has
# been created then list it in this variable.
#
# If the variable is left empty then it uses the value in STORAGE_CLASS_NAME.
#
# Example: longhorn-xfs
#
export MILVUS_KAFKA_STORAGE_CLASS_NAME=


############################################################################
#   Ollama Variables
############################################################################

# Version of Ollama to Install
#
# If left empty the latest version will be installed.
#
export OLAMA_VERSION=


# URL to Access the Ollama API if Configuring an Ingress
#
export OLLAMA_INGRESS_HOST=${CLUSTER_NAME}.${DOMAIN_NAME}


# First LLM for Ollama to Download
#
# At minimum this must be set so that a model is downloaded during deployment.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_0=llama3.2:3b


# Second LLM for Ollama to Download
#
# Leave empty if you do not want an addtional model.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_1=gemma2:2b


# Third LLM for Ollama to Download
#
# Leave empty if you do not want an addtional model.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_2=


# Fourth LLM for Ollama to Download
#
# Leave empty if you do not want an addtional model.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_3=


# Fifth LLM for Ollama to Download
#
# Leave empty if you do not want an addtional model.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_4=


############################################################################
#   Cert-Manager Variables
############################################################################

# Cert-Manager Helm Install URL
# 
# The Helm repo to install Cert-Manager from.
#
export CERTMANAGER_HELM_REPO="https://charts.jetstack.io"


# Cert-Manager Helm Chart
#
# If this variable is set then the CERTMANAGER_HELM_REPO is ignored and 
# this chart is used to install Cert-Manager instead.
#
export CERTMANAGER_HELM_CHART=oci://dp.apps.rancher.io/charts/cert-manager


# Version of Cert-Manager to Install
#
# If left empty the latest version will be installed.
#
export CERTMANAGER_VERSION=
export CERTMANAGER_VERSION=""


# Namespace to install Cert-Manager into
#
export CERTMANAGER_NAMESPACE=suse-ai


############################################################################
#   Open WebUI Variables
############################################################################

# Version of Open WebUI to Install
#
# If left empty the latest version will be installed.
#
export OWUI_VERSION=


#========  TLS Values  ========#


# The source for the TLS certificates
#
# Options:
#       suse-private-ai  (Cert-Manager will generate self-signed certificates)
#       letsEncrypt      (Cert-Manager uses letsEncrypt to generate certificates)
#       secret           (A K8s secret containing the certificate will be used)
#
export OWUI_TLS_SOURCE=suse-private-ai


# The email to use with letsEncrypt when it is used to generate certificates
#
# You must set this to something valid when using OWUI_TLS_SOURCE=letsEncrypt
#
export OWUI_TLS_EMAIL=admin@example.com


# The Lets Encrypt environment to create the certificates in
#
# Options:
#          staging  
#
export OWUI_TLS_LETSENCRYPT_ENVIRONMENT=staging


# The ingress class to use for the certificates
#
# Options: (none at the moment)
#
# Leave this variable empty unless otherwise instructed.
#
export OWUI_TLS_INGRESS_CLASS=


# Enable additional trusted certificates
#
# Options: [true|false]
# 
# Default: false
#
# This msut be set to 'true' when OWUI_TLS_SOURCE=secret
#
# If set to 'true' then a secret named tls-ca-additional must exist in the 
# SUSE AI namespace containing the additional certificates
#
# With other sources, if in doubt, set it to 'false'
#
export OWUI_TLS_ADDITIONAL_TRUSTED_CERTS=false


#========  Other Values  ========#


# Enable Open WebUI to install Ollama
#
# If enabled (set to True) Ollama will be deployed as part of the Open WebUI 
# deployment and that is the instance Open WebUI will use.
# If Ollama was previous deployed separatly and you want Open WebUI to use it
# instead of an instance it deploys set this to False.
#
# Default: True
#
export OWUI_OLLAMA_ENABLED=true


# URL Used to Access the Open WebUI Web Interface
#
export WEBUI_INGRESS_HOST=${CLUSTER_NAME}.${DOMAIN_NAME}


############################################################################
#   OpenTelemetry Collector Variables
############################################################################

# Version of OpenTelemetry Collector to Install
#
# If left empty the latest version will be installed.
#
export OTEL_VERSION=


# URL of the OpenTelemetry Collector Helm Repository
#
export OTEL_HELM_REPO_URL=https://open-telemetry.github.io/opentelemetry-helm-charts


# Namspace that Will Contain OpenTelemetry Collector
#
export OTEL_NAMESPACE=opentelemetry


############################################################################
#   Observability Agent Variables
############################################################################

# FQDN of the SUSE Observability
#
export OBSERVABILITY_HOST=observability.example.com


# URL of the Observability Agent Helm Repository
#
export OBSERVABILITY_HELM_REPO_URL=https://charts.rancher.com/server-charts/prime/suse-observability


# Observability Receiver API Key
#
# Receiver API Key created when a StackPack instance for the cluster 
# was created.
#
# It can be located immediately after the new instance is created here:
#
#   Hamburger menu (top Left) 
#         --> StackPacks 
#              --> Kubernetes 
#                   --> <INSTANCE_NAME>
#                        --> Instance Credentials 
#                             --> API Key
#
# Or can be found in the "helm upgrade --install" command listed under:
#
#   Hamburger menu (top Left) 
#         --> StackPacks 
#              --> Kubernetes 
#                   --> <INSTANCE_NAME>
#                        --> Generic Kubernetes (including RKE2)
#                             --> Step 2
#                                    helm upgrade --install \
#                                    ...
#                 (This line --->)   --set-string 'stackstate.apiKey'=
#                                    ...
#
export OBSERVABILITY_RECEIVER_API_KEY=


